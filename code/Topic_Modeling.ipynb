{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKyy26WeXmsc"
   },
   "source": [
    "'''\n",
    "Created on Sep 20, 2019\n",
    "\n",
    "@author: Gias\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IJsi5WbAuGu"
   },
   "source": [
    "Topic Model Guide\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u827xHgl-9gL"
   },
   "source": [
    "Connect to drive to get and send data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IVX7Ax2_VklN",
    "outputId": "83c5af36-0513-4b20-885e-87fec82b5e66"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2kYnDPT_DMk"
   },
   "source": [
    "Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "puwaui7zXtBd",
    "outputId": "c91e3c8c-e750-401d-a43d-6aa172c6e93e"
   },
   "outputs": [],
   "source": [
    "!pip install hickle\n",
    "!pip install tinydb\n",
    "!pip install django\n",
    "!pip install corpus\n",
    "!pip install pyLDAvis\n",
    "\n",
    "# !wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "# !unzip mallet-2.0.8.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install -U pip setuptools wheel\n",
    "# !{sys.executable} -m pip install -U spacy\n",
    "# !{sys.executable} -m spacy download en_core_web_sm\n",
    "# !{sys.executable} -m spacy download en\n",
    "# !{sys.executable} -m pip install gensim==3.8.3\n",
    "# !{sys.executable} -m pip uninstall gensim -y pyserial\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5_vwalW_F0z"
   },
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkJqhT8wXfEL",
    "outputId": "1c128f20-3a07-4d45-fa35-5205793d4daa"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pickle\n",
    "from pprint import pprint\n",
    "from lxml import etree\n",
    "import hickle as hkl\n",
    "from tinydb import TinyDB, Query\n",
    "import marshal\n",
    "from bs4 import BeautifulSoup\n",
    "#from nltk import word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "#from nltk.corpus import stopwords \n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('gutenberg')\n",
    "# nltk.download('genesis')\n",
    "# nltk.download('inaugural')\n",
    "# nltk.download('nps_chat')\n",
    "# nltk.download('webtext')\n",
    "# nltk.download('treebank')\n",
    "from nltk.book import *\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "import csv\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "dictSW = []\n",
    "for w in stopwords:\n",
    "    dictSW.append(w)\n",
    "malletStopwords = set()\n",
    "try:\n",
    "#     malletStopLists = \"/content/mallet-2.0.8/stoplists/en.txt\"\n",
    "    malletStopLists = \"mallet-2.0.8/stoplists/en.txt\"\n",
    "    with open(malletStopLists, 'rb') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            malletStopwords.add(line.strip())\n",
    "    for w in malletStopwords:\n",
    "        w = str(w).strip(\"b'\")\n",
    "        dictSW.append(w)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "from urllib.parse import urlparse, urlunsplit, urlsplit\n",
    "import unicodedata\n",
    "import nltk\n",
    "import django\n",
    "from django.conf import settings\n",
    "import shutil\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as gensim_corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# update this path where Mallet is downloaded.\n",
    "#os.environ['MALLET_HOME'] = r'C:\\dev\\opinion\\nlpmodels\\mallet-2.0.8'\n",
    "# os.environ['MALLET_HOME'] = \"/content/mallet-2.0.8\" #CHANGE\n",
    "\n",
    "#mallet_path = r'C:\\\\dev\\opinion\\nlpmodels\\mallet-2.0.8\\bin\\mallet' # update this path\n",
    "# mallet_path = \"/content/mallet-2.0.8/bin/mallet\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BS4zHBhsl9D8"
   },
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKLnHoQ1_J-s"
   },
   "source": [
    "Add words to blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1kBm5zq9NvEP"
   },
   "outputs": [],
   "source": [
    "dictSW.append('vulnerability')\n",
    "dictSW.append('vulnerable')\n",
    "dictSW.append('due')\n",
    "dictSW.append('include')\n",
    "dictSW.append('aka')\n",
    "dictSW.append('remote')\n",
    "dictSW.append('attacker')\n",
    "dictSW.append('attackers')\n",
    "dictSW.append('attack')\n",
    "dictSW.append('quot')\n",
    "dictSW.append('would')\n",
    "dictSW.append('could')\n",
    "dictSW.append('able')\n",
    "dictSW.append('vulnerabilities')\n",
    "dictSW.append('Microsoft')\n",
    "dictSW.append('Oracle')\n",
    "dictSW.append('oracle')\n",
    "dictSW.append('Windows')\n",
    "dictSW.append('Android')\n",
    "dictSW.append('wordpress')\n",
    "dictSW.append('Version')\n",
    "dictSW.append('version')\n",
    "dictSW.append('Apple')\n",
    "dictSW.append('apple')\n",
    "dictSW.pop(0)\n",
    "\n",
    "print(len(dictSW))\n",
    "\n",
    "dictSW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzN9RPRu_WEk"
   },
   "source": [
    "Get all descriptions from NVD with 0.95 similarity or less.\n",
    "Get CWEs, CVEs and the number of NVDs per year.\n",
    "Words are cleaned, stemmed, and lemmatized to obtain a corpus and id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJbYOMq4ohTI"
   },
   "outputs": [],
   "source": [
    "def clean_string(text):\n",
    "  text = ''.join([word for word in text if word not in string.punctuation])\n",
    "  text = text.lower()\n",
    "  text = ' '.join([word for word in text.split() if word not in dictSW])\n",
    "\n",
    "  return text\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stopwords] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "#db= \"/content/drive/MyDrive/DISA Labs Research 2021/Vulnerability Databases/New_NVD/0.8/Cosine_NVD_2011-2021_0.8.csv\"\n",
    "# db = \"/content/drive/MyDrive/Vulnerability Databases/New_NVD/0.95/Cosine_NVD_2011-2021_0.95_reject.csv\"\n",
    "db = \"NVD_2011-2021.csv\"\n",
    "\n",
    "fields = []\n",
    "rows = []\n",
    "\n",
    "cve = []\n",
    "cwe = []\n",
    "data = []\n",
    "\n",
    "years = {\n",
    "    \"2011\": 0,\n",
    "    \"2012\": 0,\n",
    "    \"2013\": 0,\n",
    "    \"2014\": 0,\n",
    "    \"2015\": 0,\n",
    "    \"2016\": 0,\n",
    "    \"2017\": 0,\n",
    "    \"2018\": 0,\n",
    "    \"2019\": 0,\n",
    "    \"2020\": 0,\n",
    "    \"2021\": 0,\n",
    "}\n",
    "\n",
    "# Get rows from designated CSV file\n",
    "\n",
    "with open(db, 'r', encoding='utf-8-sig') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    fields = next(csvreader)\n",
    "    for row in csvreader: \n",
    "        rows.append(row)\n",
    "    \n",
    "    for j in rows:\n",
    "        if j[2].find(\"** REJECT **\") == -1:\n",
    "            cve.append(j[0])\n",
    "            data.append(j[2])\n",
    "            cwe.append(j[4])\n",
    "            years.update({j[1].split(\"/\")[2]: (years[j[1].split(\"/\")[2]] + 1)})\n",
    "\n",
    "print(\"Complete\")\n",
    "print(len(data))\n",
    "\n",
    "cleaned = list(map(clean_string, data))\n",
    "# print(cleaned[0])\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(cleaned))\n",
    "\n",
    "print(\"data_words: \", len(data_words))\n",
    "print(data_words[0])\n",
    "\n",
    "# stem_words = []\n",
    "\n",
    "# for i in data_words:\n",
    "#     stem_words.append(list(map(ps.stem, i))) #ps is porter stemmer\n",
    "\n",
    "# print(\"stemming\")\n",
    "# print(stem_words[0])\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# see a trigram example\n",
    "# print(trigram_mod[bigram_mod[data_words[7]]])\n",
    "\n",
    "# (1) without stemming \n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])\n",
    "\n",
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n",
    "\n",
    "# (2) with stemming\n",
    "# # Form Bigrams\n",
    "# data_words_bigrams = make_bigrams(stem_words)\n",
    "\n",
    "# # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# # python3 -m spacy download en\n",
    "# nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# # Do lemmatization keeping only noun, adj, vb, adv\n",
    "# data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "# # Create Dictionary\n",
    "# id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# # Create Corpus\n",
    "# texts = data_lemmatized\n",
    "\n",
    "# # Term Document Frequency\n",
    "# corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# # View\n",
    "# print(corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtjBPtkwATPm"
   },
   "source": [
    "Get LDA mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N-D7a09iCRIH",
    "outputId": "38aa5097-1182-46cf-95df-a210fddc95dd"
   },
   "outputs": [],
   "source": [
    "# os.environ.update({'MALLET_HOME':r'PATH'}) #CHANGE\n",
    "# mallet_path = r'PATH' #CHANGE\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=18, id2word=id2word)\n",
    "# Show Topics\n",
    "pprint(ldamallet.show_topics(18, formatted=False))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKCS1GWuAq5W"
   },
   "source": [
    "Get the dominant topic of each description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WH5ozcDF8rEJ",
    "outputId": "ca5e30cf-f36f-4408-fd38-cb561cf144e9"
   },
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=data, cveids=cve, cwes=cwe):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    \n",
    "    # Add original cveids to the end of the output\n",
    "    contents2 = pd.Series(cveids)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents2], axis=1)\n",
    "\n",
    "    # Add original cwes to the end of the output\n",
    "    contents3 = pd.Series(cwes)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents3], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=data, cveids=cve, cwes=cwe)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text', 'CVE-id', 'CWE']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)\n",
    "print(df_dominant_topic.to_csv(\"labelling.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jz2OW1W8nIaZ"
   },
   "source": [
    "SECTION 1: Label topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtYkLhywYGVb"
   },
   "outputs": [],
   "source": [
    "all_topics = []\n",
    "\n",
    "Topics = {\n",
    "    \"Topic ID\": None,\n",
    "    \"Topic Label\": None,\n",
    "    \"Topic Description\": None,\n",
    "    \"Document Link\": None,\n",
    "    \"Topic Keywords\": None\n",
    "}\n",
    "\n",
    "def updateTopic(id, label, desc, link, key):\n",
    "    Topics.update({\"Topic ID\": id})\n",
    "    Topics.update({\"Topic Label\": label})\n",
    "    Topics.update({\"Topic Description\": desc})\n",
    "    Topics.update({\"Document Link\": link})\n",
    "    Topics.update({\"Topic Keywords\": key})\n",
    "\n",
    "    all_topics.append(Topics.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55kkDR_xE86j"
   },
   "outputs": [],
   "source": [
    "all_names = []\n",
    "\n",
    "CVE_name = {\n",
    "    \"number\": None,\n",
    "    \"name\": None,\n",
    "    \"description\": None,\n",
    "}\n",
    "\n",
    "def fillName(number, name, description):\n",
    "    CVE_name.update({\"number\": number})\n",
    "    CVE_name.update({\"name\": name})\n",
    "    CVE_name.update({\"description\": description})\n",
    "    all_names.append(CVE_name.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQ3hRQx7F20E"
   },
   "outputs": [],
   "source": [
    "fillName(\"682\", \"Incorrect Calculation\", \"The software performs a calculation that generates incorrect or unintended results that are later used in security-critical decisions or resource management.\")\n",
    "fillName(\"688\", \"Exposure of Resource to Wrong Sphere\", \"The product exposes a resource to the wrong control sphere, providing unintended actors with inappropriate access to the resource.\")\n",
    "fillName(\"532\", \"Insertion of Sensitive Information into Log File\", \"Information written to log files can be of a sensitive nature and give valuable guidance to an attacker or expose sensitive user information.\")\n",
    "fillName(\"693\", \"Protection Mechanism Failure\", \"The product does not use or incorrectly uses a protection mechanism that provides sufficient defense against directed attacks against the product.\")\n",
    "fillName(\"369\", \"Divide By Zero\", \"The product divides a value by zero.\")\n",
    "fillName(\"131\", \"Incorrect Calculation of Buffer Size\", \"The software does not correctly calculate the size to be used when allocating a buffer, which could lead to a buffer overflow.\")\n",
    "fillName(\"788\", \"Access of Memory Location After End of Buffer\", \"The software reads or writes to a buffer using an index or pointer that references a memory location after the end of the buffer.\")\n",
    "fillName(\"121\", \"Stack-based Buffer Overflow\", \"A stack-based buffer overflow condition is a condition where the buffer being overwritten is allocated on the stack (i.e., is a local variable or, rarely, a parameter to a function).\")\n",
    "fillName(\"843\", \"Access of Resource Using Incompatible Type ('Type Confusion')\", \"The program allocates or initializes a resource such as a pointer, object, or variable using one type, but it later accesses that resource using a type that is incompatible with the original type.\")\n",
    "fillName(\"362\", \"Concurrent Execution using Shared Resource with Improper Synchronization ('Race Condition')\", \"The program contains a code sequence that can run concurrently with other code, and the code sequence requires temporary, exclusive access to a shared resource, but a timing window exists in which the shared resource can be modified by another code sequence that is operating concurrently.\")\n",
    "fillName(\"732\", \"Incorrect Permission Assignment for Critical Resource\", \"The product specifies permissions for a security-critical resource in a way that allows that resource to be read or modified by unintended actors.\")\n",
    "fillName(\"1021\", \"Improper Restriction of Rendered UI Layers or Frames\", \"The web application does not restrict or incorrectly restricts frame objects or UI layers that belong to another application or domain, which can lead to user confusion about which interface the user is interacting with.\")\n",
    "fillName(\"1188\", \"Insecure Default Initialization of Resource\", \"The software initializes or sets a resource with a default that is intended to be changed by the administrator, but the default is not secure.\")\n",
    "fillName(\"346\", \"Origin Validation Error\", \"The software does not properly verify that the source of data or communication is valid.\")\n",
    "fillName(\"59\", \"Improper Link Resolution Before File Access ('Link Following')\", \"The software attempts to access a file based on the filename, but it does not properly prevent that filename from identifying a link or shortcut that resolves to an unintended resource.\")\n",
    "fillName(\"611\", \"Improper Restriction of XML External Entity Reference\", \"The software processes an XML document that can contain XML entities with URIs that resolve to documents outside of the intended sphere of control, causing the product to embed incorrect documents into its output.\")\n",
    "fillName(\"295\", \"Improper Certificate Validation\", \"The software does not validate, or incorrectly validates, a certificate.\")\n",
    "fillName(\"770\", \"Allocation of Resources Without Limits or Throttling\", \"The software allocates a reusable resource or group of resources on behalf of an actor without imposing any restrictions on the size or number of resources that can be allocated, in violation of the intended security policy for that actor.\")\n",
    "fillName(\"119\", \"Improper Restriction of Operations within the Bounds of a Memory Buffer\", \"The software performs operations on a memory buffer, but it can read from or write to a memory location that is outside of the intended boundary of the buffer.\")\n",
    "fillName(\"190\", \"Integer Overflow or Wraparound\", \"The software performs a calculation that can produce an integer overflow or wraparound, when the logic assumes that the resulting value will always be larger than the original value. This can introduce other weaknesses when the calculation is used for resource management or execution control.\")\n",
    "fillName(\"129\", \"Improper Validation of Array Index\", \"The product uses untrusted input when calculating or using an array index, but the product does not validate or incorrectly validates the index to ensure the index references a valid position within the array.\")\n",
    "fillName(\"74\", \"Improper Neutralization of Special Elements in Output Used by a Downstream Component ('Injection')\", \"The software constructs all or part of a command, data structure, or record using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify how it is parsed or interpreted when it is sent to a downstream component.\")\n",
    "fillName(\"404\", \"Improper Resource Shutdown or Release\", \"The program does not release or incorrectly releases a resource before it is made available for re-use.\")\n",
    "fillName(\"502\", \"Deserialization of Untrusted Data\", \"The application deserializes untrusted data without sufficiently verifying that the resulting data will be valid.\")\n",
    "fillName(\"642\", \"External Control of Critical State Data\", \"The software stores security-critical state information about its users, or the software itself, in a location that is accessible to unauthorized actors.\")\n",
    "fillName(\"327\", \"Use of a Broken or Risky Cryptographic Algorithm\", \"The use of a broken or risky cryptographic algorithm is an unnecessary risk that may result in the exposure of sensitive information.\")\n",
    "fillName(\"294\", \"Authentication Bypass by Capture-replay\", \"A capture-replay flaw exists when the design of the software makes it possible for a malicious user to sniff network traffic and bypass authentication by replaying it to the server in question to the same effect as the original message (or with minor changes).\")\n",
    "fillName(\"200\", \"Exposure of Sensitive Information to an Unauthorized Actor\", \"The product exposes sensitive information to an actor that is not explicitly authorized to have access to that information.\")\n",
    "fillName(\"330\", \"Use of Insufficiently Random Values\", \"The software uses insufficiently random numbers or values in a security context that depends on unpredictable numbers.\")\n",
    "fillName(\"863\", \"Incorrect Authorization\", \"The software performs an authorization check when an actor attempts to access a resource or perform an action, but it does not correctly perform the check. This allows attackers to bypass intended access restrictions.\")\n",
    "fillName(\"347\", \"Improper Verification of Cryptographic Signature\", \"The software does not verify, or incorrectly verifies, the cryptographic signature for data.\")\n",
    "fillName(\"312\", \"Cleartext Storage of Sensitive Information\", \"The application stores sensitive information in cleartext within a resource that might be accessible to another control sphere.\")\n",
    "fillName(\"79\", \"Cross-site Scripting\", \"The software does not neutralize or incorrectly neutralizes user-controllable input before it is placed in output that is used as a web page that is served to other users.\")\n",
    "fillName(\"269\", \"Improper Priviledge Management\", \"The software does not properly assign, modify, track, or check privileges for an actor, creating an unintended sphere of control for that actor.\")\n",
    "fillName(\"20\", \"Improper Input Validation\", \"The product receives input or data, but it does not validate or incorrectly validates that the input has the properties that are required to process the data safely and correctly.\")\n",
    "fillName(\"310\", \"Cryptographic Issues\", \"Weaknesses in this category are related to the design and implementation of data confidentiality and integrity. Frequently these deal with the use of encoding techniques, encryption libraries, and hashing algorithms. The weaknesses in this category could lead to a degradation of the quality data if they are not addressed.\")\n",
    "fillName(\"89\", \"SQL Injection\", \"The software constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component.\")\n",
    "fillName(\"120\", \"Buffer Overflow\", \"The program copies an input buffer to an output buffer without verifying that the size of the input buffer is less than the size of the output buffer, leading to a buffer overflow.\")\n",
    "fillName(\"287\", \"Improper Authentication\", \"When an actor claims to have a given identity, the software does not prove or insufficiently proves that the claim is correct.\")\n",
    "fillName(\"416\", \"Use After Free\", \"Referencing memory after it has been freed can cause a program to crash, use unexpected values, or execute code.\")\n",
    "fillName(\"400\", \"Uncontrolled Resource Consumption\", \"The software does not properly control the allocation and maintenance of a limited resource, thereby enabling an actor to influence the amount of resources consumed, eventually leading to the exhaustion of available resources.\")\n",
    "fillName(\"434\", \"Unrestricted Upload of File with Dangerous Type\", \"The software allows the attacker to upload or transfer files of dangerous types that can be automatically processed within the product's environment.\")\n",
    "fillName(\"476\", \"Null Pointer Derefence\", \"A NULL pointer dereference occurs when the application dereferences a pointer that it expects to be valid, but is NULL, typically causing a crash or exit.\")\n",
    "fillName(\"22\", \"Path Traversal\", \"The software uses external input to construct a pathname that is intended to identify a file or directory that is located underneath a restricted parent directory, but the software does not properly neutralize special elements within the pathname that can cause the pathname to resolve to a location that is outside of the restricted directory.\")\n",
    "fillName(\"77\", \"Command Injection\", \"The software constructs all or part of a command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended command when it is sent to a downstream component.\")\n",
    "fillName(\"787\", \"Out-Of-Bounds Write\", \"The software writes data past the end, or before the beginning, of the intended buffer.\")\n",
    "fillName(\"444\", \"HTTP Request Smuggling\", \"When malformed or abnormal HTTP requests are interpreted by one or more entities in the data flow between the user and the web server, such as a proxy or firewall, they can be interpreted inconsistently, allowing the attacker to \\\"smuggle\\\" a request to one device without the other device being aware of it.\")\n",
    "fillName(\"78\", \"OS Command Injection\", \"The software constructs all or part of an OS command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended OS command when it is sent to a downstream component.\")\n",
    "fillName(\"601\", \"Open Redirect\", \"A web application accepts a user-controlled input that specifies a link to an external site, and uses that link in a Redirect. This simplifies phishing attacks.\")\n",
    "fillName(\"754\", \"Improper Check for Unusual or Exceptional Conditions\", \"The software does not check or incorrectly checks for unusual or exceptional conditions that are not expected to occur frequently during day to day operation of the software.\")\n",
    "fillName(\"284\", \"Improper Access Control\", \"The software does not restrict or incorrectly restricts access to a resource from an unauthorized actor.\")\n",
    "fillName(\"125\", \"Out-of-bounds Read\", \"The software reads data past the end, or before the beginning, of the intended buffer.\")\n",
    "fillName(\"306\", \"Missing Authentication for Critical Function\", \"The software does not perform any authentication for functionality that requires a provable user identity or consumes a significant amount of resources.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tXbhpguG89AJ",
    "outputId": "eb58c246-7966-4cb1-a1cb-d7bc3bbb440c"
   },
   "outputs": [],
   "source": [
    "arr2 = []\n",
    "\n",
    "diction2 = {\n",
    "    \"Document_ID\": None,\n",
    "    \"Topic_ID\": None,\n",
    "    \"Correlation_Value\": None,\n",
    "}\n",
    "\n",
    "def update(id1, id2, value):\n",
    "    diction2.update({\"Document_ID\": id1})\n",
    "    diction2.update({\"Topic_ID\": id2})\n",
    "    diction2.update({\"Correlation_Value\": value})\n",
    "\n",
    "    arr2.append(diction2.copy())\n",
    "\n",
    "# Group top 15 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "#show\n",
    "sent_topics_sorteddf_mallet\n",
    "\n",
    "topics = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "all_numbers = []\n",
    "counter = 0\n",
    "index = 0\n",
    "once = True\n",
    "top_num = 8\n",
    "\n",
    "\n",
    "for i in range(len(sent_topics_sorteddf_mallet)):\n",
    "  if counter == 0:\n",
    "    index = topics.pop(0)\n",
    "  print(\"i\", i)\n",
    "  if sent_topics_sorteddf_mallet['Topic_Num'][i] == float(0): #float(0) ?\n",
    "    print(sent_topics_sorteddf_mallet['Topic_Num'][i])\n",
    "    print(sent_topics_sorteddf_mallet['Topic_Perc_Contrib'][i])\n",
    "#     print(sent_topics_sorteddf_mallet['Keywords'][i])\n",
    "#     print(sent_topics_sorteddf_mallet['Text'][i])\n",
    "    for j in range(len(data)):\n",
    "          if data[j] == sent_topics_sorteddf_mallet['Text'][i]:\n",
    "            print(cwe[j])\n",
    "            for n in all_names:\n",
    "                if type(n) is not str:\n",
    "                    if cwe[j] != \"NVD-CWE-noinfo\" and cwe[j] != \"NVD-CWE-Other\" and cwe[j] != \"Unavailable\":\n",
    "                        if n['number'] == cwe[j].split(\"/\")[5].split(\".\")[0]:\n",
    "                            print(n[\"name\"], n['number'])\n",
    "                            all_numbers.append(n[\"number\"])\n",
    "    print()\n",
    "  print(\"counter\", counter)\n",
    "  counter += 1\n",
    "  if counter == 15:\n",
    "    counter = 0\n",
    "    all_numbers\n",
    "    # Pass the split_it list to instance of Counter class.\n",
    "    counter = Counter(all_numbers)\n",
    "  \n",
    "    # most_common() produces k frequently encountered\n",
    "    # input values and their respective counts.\n",
    "    most_occur = counter.most_common(4)\n",
    "  \n",
    "    print(most_occur)\n",
    "\n",
    "    #updateTopic(index, n[\"name\"], n[\"description\"], cwe[j], sent_topics_sorteddf_mallet['Keywords'][i])\n",
    "    once = False\n",
    "\n",
    "# #fields = [\"Topic ID\", \"Topic Label\", \"Topic Description\", \"Document Link\", \"Topic Keywords\"]\n",
    "# #filename = \"/content/Sheet1.csv\"\n",
    "\n",
    "# #with open(filename2, 'w') as csvfile:\n",
    "# #  writer = csv.DictWriter(csvfile, fieldnames = fields2)\n",
    "# #  writer.writeheader()\n",
    "# #  writer.writerows(arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_twkFN7IiVhe"
   },
   "outputs": [],
   "source": [
    "updateTopic(8, \"Cleartext Storage of Sensitive Information\", \"The application stores sensitive information in cleartext within a resource that might be accessible to another control sphere.\", \"http://cwe.mitre.org/data/definitions/312.html\", \"page, store, user, site, session, content, conduct, victim, reflect, field\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jYTDynmDr3bz"
   },
   "outputs": [],
   "source": [
    "fields2 = [\"Topic ID\", \"Topic Label\", \"Topic Description\", \"Document Link\", \"Topic Keywords\"]\n",
    "filename2 = \"/content/Sheet3.csv\"\n",
    "\n",
    "with open(filename2, 'w') as csvfile:\n",
    "  writer = csv.DictWriter(csvfile, fieldnames = fields2)\n",
    "  writer.writeheader()\n",
    "  writer.writerows(all_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OCbNAUf_eZW9",
    "outputId": "2e7d3526-2b1d-410f-983e-08ede894b8c3"
   },
   "outputs": [],
   "source": [
    "all_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NQ-LLn9nHtG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "c8pP3OSb4OBV",
    "outputId": "b4ae06d3-9a3c-4f9e-98d6-6ba64917e3fd"
   },
   "outputs": [],
   "source": [
    "all_years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n",
    "\n",
    "interval = []\n",
    "\n",
    "for year in all_years:\n",
    "  num = 0\n",
    "  num += interval[len(interval) - 1] if len(interval) is not 0 else 0\n",
    "  num += years[str(year)]\n",
    "  interval.append(num)\n",
    "\n",
    "# interval\n",
    "total_importance = []\n",
    "\n",
    "row = sorted(ldamallet[corpus], key=lambda x: (x[0]))\n",
    "\n",
    "# for i in range(len(row)):\n",
    "#     temp_row.append(i)\n",
    "#     for j in range(len[row[i]]):\n",
    "#         temp_row.append(row[i][j])\n",
    "# with open('Question_Information.csv', 'a',encoding='utf-8') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     temp_row = []\n",
    "\n",
    "#     writer.writerow([])\n",
    "\n",
    "\n",
    "# print(row[0][0][1]) #0.010582010582010583\n",
    "# def importance(n, y):\n",
    "#   return (n / y) * 100\n",
    "\n",
    "# for k in range(len(all_years)):\n",
    "#   topic_importance = [0]*20\n",
    "#   for i in range(0 if all_years[k] == 2011 else interval[k - 1], interval[k]):\n",
    "#     for j in range(20):\n",
    "#       topic_importance[j] += row[i][j][1]\n",
    "#   temp_year = [years[str(all_years[k])]]*20\n",
    "#   year_importance = list(map(importance, topic_importance, temp_year))\n",
    "#   total_importance.append(year_importance)\n",
    "\n",
    "# total_importance\n",
    "\n",
    "# yearsss = {\n",
    "#     \"2011\": None,\n",
    "#     \"2012\": None,\n",
    "#     \"2013\": None,\n",
    "#     \"2014\": None,\n",
    "#     \"2015\": None,\n",
    "#     \"2016\": None,\n",
    "#     \"2017\": None,\n",
    "#     \"2018\": None,\n",
    "#     \"2019\": None,\n",
    "#     \"2020\": None,\n",
    "#     \"2021\": None,\n",
    "# }\n",
    "\n",
    "# #dict_arr_years = []\n",
    "\n",
    "# for i in total_importance:\n",
    "#   for j in range(11):\n",
    "#     yearsss.update({str(all_years[j]): i[j]})\n",
    "#   dict_arr_years.append(yearsss.copy())\n",
    "\n",
    "# lastYear = [\"2011\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\"]\n",
    "\n",
    "# #dict_arr_years\n",
    "# #lastYear\n",
    "\n",
    "# # with open(\"/content/importance.csv\", 'w') as csvfile:\n",
    "# #   writer = csv.DictWriter(csvfile, lastYear)\n",
    "# #   writer.writeheader()\n",
    "# #   writer.writerows(dict_arr_years)\n",
    "# with open(\"importance.csv\", 'w') as csvfile:\n",
    "#   writer = csv.DictWriter(csvfile, lastYear)\n",
    "#   writer.writeheader()\n",
    "#   writer.writerows(dict_arr_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = enumerate(ldamallet[corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(sorted_data):\n",
    "    data = []\n",
    "    for s in sorted_data:\n",
    "        data.append(s[1])\n",
    "    with open('importance.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([data[0], data[1], data[2],data[3],data[4],data[5],data[6],data[7],data[8],data[9],data[10],\n",
    "                        data[11],data[12],data[13],data[14],data[15],data[16],data[17]])\n",
    "\n",
    "count = 0\n",
    "temp_row = []\n",
    "for i, row in e:\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        for j in range(18):\n",
    "            temp_row.append(row[j])\n",
    "            \n",
    "        sorted_r = sorted(temp_row)\n",
    "        save_to_file(sorted_r)\n",
    "        temp_row.clear()\n",
    "        print(count)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnsrfLFOBAj_"
   },
   "source": [
    "Number of dominant descriptions per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "MkgFtf1S9Cdx",
    "outputId": "dcca4f5e-b169-4cbe-c0eb-be03e4b59298"
   },
   "outputs": [],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "print(df_dominant_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWgyCN6A8s0g"
   },
   "source": [
    "------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyfguWkzBUJn"
   },
   "source": [
    "Get best coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 826
    },
    "id": "Vwi-lsFjknwC",
    "outputId": "fe629152-47ae-4e51-e952-b71303d6d91d"
   },
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        print(num_topics)\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "\n",
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=12, limit=45, step=3)\n",
    "\n",
    "# Show graph\n",
    "limit=45; start=12; step=3;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()\n",
    "\n",
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWDlH4AumA-H"
   },
   "source": [
    "------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TopicModeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
